{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3ca0fb38",
      "metadata": {
        "id": "3ca0fb38"
      },
      "source": [
        "# Lab 10\n",
        "# Sentiment Analysis and Text Classification with LSTM using spaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ee05bd5",
      "metadata": {
        "id": "8ee05bd5"
      },
      "source": [
        "## 14.1 Introduction\n",
        "\n",
        "This lab discusses a vital component of NLP implementation: text classification and the application on sentiment analysis.\n",
        "\n",
        "You will first learn how to train spaCy's text classifier component, TextCategorizer. For this, you will learn how to prepare data and feed the data to the classifier; then we'll proceed to train the classifier.\n",
        "\n",
        "You will also practice your new TextCategorizer skills on a popular dataset for sentiment analysis.\n",
        "\n",
        "Next, you will learn how to do text classification with two vital Python frame-works: TensorFlow Keras API and spaCy technology.\n",
        "\n",
        "You will also learn the basics of neural networks, sequential data modeling with LSTM Technology, and how to process text for machine learning tasks with Keras's text preprocessing module. You will also practice how to implement a neu-ral network with tf.keras as well.\n",
        "\n",
        "Following that, we will implement a step-by-step text classification system, starting with data acquisition to text preprocessing by using Keras tokenizer, neural network configuration, network training, testing and system evaluation.\n",
        "\n",
        "This lab will cover the following key topics:\n",
        "- Basic concept and knowledge of text classification\n",
        "- Model training of spaCy text classifier\n",
        "- Sentiment Analysis with spaCy\n",
        "- Sequential modeling with LSTM Technology\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94a8da52",
      "metadata": {
        "id": "94a8da52"
      },
      "source": [
        "## 14.2 Technical Requirements\n",
        "\n",
        "The code in the sections Training the spaCy text classifier and Sentiment analysis with spaCy is spaCy v3.0 compatible.\n",
        "\n",
        "The section Text classification with spaCy and Keras requires the following Python libraries:\n",
        "- TensorFlow (version 2.3 or above)\n",
        "- NumPy\n",
        "- pandas\n",
        "- Matplotlib\n",
        "\n",
        "If you haven't install these packages into your own PC/notebook, do that using: <font color='blue'>pip install xxx </font>command."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a77f087a",
      "metadata": {
        "id": "a77f087a"
      },
      "source": [
        "## 14.3 Text Classification in a Nutshell\n",
        "\n",
        "### What is Text Classfication?\n",
        "\n",
        "Text Classification is the task of assigning a set of predefined labels to text. By using a batch of predefined text classes or categories, such as “like” or “unlike” comments in classical sentiment analysis in Facebook. One might want to know an online comments belongs to which catego-ry or sentiment class. In the past, all these text classifications (sentiment analysis) are done by manual tagging and classification. Now, we try to machine learning technique to “train” the classification system with known examples (so-called training samples) in order to classify the “unseen” cases.\n",
        "\n",
        "For example: A customer review on a movie in IMDB can be positive, negative, or neutral.\n",
        "\n",
        "In real world applications, such Text Classification systems also commonly used for automatic detection of spam emails, determining the sentiment of cus-tomer's reviews, understanding customer's intent even sorting customer's com-plaint tickets or other practical applications.\n",
        "\n",
        "In fact, text classification is a fundamental task of NLP. Owing to the fast-growing of social media such as Facebook or Wechat. It is gaining importance in the business world, as it enables businesses to automate their processes.\n",
        "\n",
        "One immediate example is spam filters. Every day, users receive many spam emails but most of the time never see these emails and don't get any notifications because spam filters save the users from bothering about irrelevant emails and from spending time deleting these emails.\n",
        "\n",
        "Another very popular application of Text classification is sentiment analysis in popular social media such as Wechat and Facebook. By using various machine learning method such as LSTM Technology for automatic analysis of dialogues between users, the system can analyze the views/comments/opinions of certain issue such as the sentiment analysis in Facebook on the popularity of Donald Trump during presidential election years before.\n",
        "\n",
        "Text Classifiers can come in different flavors. Some classifiers focus on the overall emotion of the text, some classifiers focus on detecting the language of the text, and some classifiers focus on only some words of the text, such as verbs.\n",
        "\n",
        "The following are some of the most common types of text classification and their use cases:\n",
        "1. Language detection: Language detection is the first step of many NLP sys-tems, such as machine translation.\n",
        "2. Topic generation: Topic generation (and detection) refers to the process of the summarization (or classification) of a batch of sentence, paragraphs or texts into certain TOI (Topic of Interest) or topic title. For example, the text in a customer email could be asking about a refund, asking for a past bill, or simply complaining about the customer service.\n",
        "3. Sentiment analysis: In the world of social media, sentiment analysis is a vi-tal task to classify (or analyze) the user’s responses, comments and mes-sages on a particular topic that belongs to certain classes such as: positive, neutral or negative emotions; like or dislike, etc. Sentiment analysis is also commonly used to analyze customer reviews about products and services, now become a very essential task in e-commerce and social media.\n",
        "\n",
        "The following figure shows a text classifier for a customer service automation system (Fig. 14.1):\n",
        "\n",
        "<img src=\"./Fig 14.1.jpg\" width = \"600\" height = \"\" alt=\"Fig1\" align=center />\n",
        "Fig 14.1 Example of Top Detection for customer complain in CSAS (Customer Service Automation System)\n",
        "\n",
        "\n",
        "### Text Classification as AI Applications\n",
        "\n",
        "In terms of AI technology, text classification can be considered as Supervised-Learning (SL) AI task.\n",
        "\n",
        "It means that the classifier can predict the class label of a text based on sample input text-class label pairs.\n",
        "\n",
        "In other words, like any SL machine learning application, text classification must need sufficient databank of input(text)-output(classified labels) pairs for network training, testing and validation.\n",
        "\n",
        "Hence, to train a text classifier, we need a labeled dataset. A labeled dataset is basically a list of text-label pairs. Here is an example dataset of five training sen-tences with their labels (Fig. 14.2):\n",
        "\n",
        "<img src=\"./Fig 14.2.jpg\" width = \"600\" height = \"\" alt=\"Fig2\" align=center/>\n",
        "Fig 14.2  Sample input texts and their corresponding output class labels\n",
        "\n",
        "Then we train the classifier by showing the text and the corresponding class la-bels to the classifier.\n",
        "\n",
        "When the classifier sees new text that was not in the training text, it then predicts the class label of this unseen text based on the examples it saw during the training phase. The output of a text classifier is always a class label.\n",
        "\n",
        "Text classification can also be divided into three categories depending on the number of classes used:\n",
        "1. Binary text classification means that we want to categorize our text into two classes.\n",
        "2. Multi-class text classification means that there are more than two classes. Each class is mutually exclusive – one text can belong to one class only. Equivalently, a training instance can be labeled with only one class label. An example is rating customer reviews. A review can have 1, 2, 3, 4, or 5 stars (each star category is a class).\n",
        "3. Multi-label text classification system is a generalization its multi-class coun-terpart, whereas these multi-labels can be assigned to each example text. For example, classifying negative social media messages is done with multi-labels. This way, our model can distinguish different levels of negative emotions. Class labels (categories) are typically toxic, severe toxic, insult, threat, ob-scenity. A message can include both insults and threats, or be classed as insult, toxicity, and obscenity, and so on. Hence for this problem, using multiple classes is more suitable.\n",
        "\n",
        "\n",
        "\n",
        "### What are Labels in Text Classification?\n",
        "\n",
        "Labels are the name of the classes we want to see as the output.\n",
        "\n",
        "A class label can be categorical (string) or numerical (a number).\n",
        "\n",
        "Here are some commonly used class labels in Text Classfication:\n",
        "- For sentiment analysis, we usually use the class labels positive and negative. Their abbreviations, pos and neg, are also commonly used. Binary class labels are popular as well – 0 means negative sentiment and 1 means positive sentiment.\n",
        "- The same applies to binary classification problems. We usually use 0-1 for class labels.\n",
        "- For multiclass and multilabel problems, we usually name the classes with a meaningful name. For a movie genre classifier, we can use the labels family, international, Sunday evening, Disney, action, and so on. Numbers are used as labels as well. For a five-class classification problem, we can use the labels 1, 2, 3, 4, and 5.\n",
        "\n",
        "Now we've covered the basic concepts of text classification, let's do some coding! In the next section, we'll explore how to train spaCy's text classifier component."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39db478a",
      "metadata": {
        "id": "39db478a"
      },
      "source": [
        "## 14.4 Training Text Classifier with spaCy NLP Pipeline\n",
        "\n",
        "In this section, we will learn about the details of spaCy's text classifier component TextCategorizer (tCategorizer).\n",
        "\n",
        "TextCategorizer is an optional and trainable pipeline component. In order to train it, we need to provide examples and their class labels. We first add TextCat-egorizer to the NLP pipeline and then do the training procedure.\n",
        "\n",
        "Fig. 14.3 shows where exactly the TextCategorizer component lies in the NLP pipeline; this component comes after the essential components.\n",
        "\n",
        "In the following diagram, textcat refers to the TextCategorizer component.\n",
        "\n",
        "<img src=\"./Fig 14.3.jpg\" width = \"600\" height = \"\" alt=\"Fig3\" align=center />\n",
        "Fig 14.3 TextCategorizer in the spaCy NLP pipeline\n",
        "\n",
        "In practice, a neural network architecture lies behind spaCy's TextCategorizer.\n",
        "\n",
        "TextCategorizer provides us with user-friendly and end-to-end approaches to train the classifier, so that we don't have to deal with the neural network architec-ture directly. We'll design our own neural network architecture in the upcoming Text classification with spaCy and Keras section.\n",
        "\n",
        "After looking at the architecture, we’re ready to dive into TextCategorizer code. Let’s get to know TextCategorizer class first.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "950785ae",
      "metadata": {
        "id": "950785ae"
      },
      "source": [
        "### 14.4.1 TextCategorizer class\n",
        "\n",
        "Now let's get to know the <font color='blue'>TextCategorizer</font> class in detail.\n",
        "\n",
        "First thing first, import spaCy and load the nlp component from \"en_core_web_md\" first."
      ]
    },
    {
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ga9AHP1vTTiN",
        "outputId": "20f30021-9f26-49b5-9e99-e7cf4e4f314f"
      },
      "id": "Ga9AHP1vTTiN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29801878",
      "metadata": {
        "id": "29801878"
      },
      "outputs": [],
      "source": [
        "# Load and import spacy package\n",
        "import spacy\n",
        "# Load the en_core_web_md module\n",
        "nlp = spacy.load(\"en_core_web_md\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb894cbf",
      "metadata": {
        "id": "bb894cbf"
      },
      "source": [
        "Next, we import <font color='blue'>TextCategorizer</font> from the <font color='blue'>spaCy</font> pipeline components:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0337b54c",
      "metadata": {
        "id": "0337b54c"
      },
      "outputs": [],
      "source": [
        "# Import the Single Textcat Model\n",
        "from spacy.pipeline.textcat import DEFAULT_SINGLE_TEXTCAT_MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e83b9151",
      "metadata": {
        "id": "e83b9151"
      },
      "source": [
        "<font color='blue'>TextCategorizer</font> is available in two flavors, <font color='blue'>single-label classifier</font> and <font color='blue'>multilabel classifier</font>.\n",
        "\n",
        "As we remarked in the previous section, a multilabel classifier can predict more than one class. A single-label classifier predicts only one class for each example and classes are mutually exclusive.\n",
        "\n",
        "The preceding import line imports the single-label classifier and the following code imports the multilabel classifier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9814e843",
      "metadata": {
        "id": "9814e843"
      },
      "outputs": [],
      "source": [
        "# Import the Mutli-TextCat Model\n",
        "from spacy.pipeline.textcat_multilabel import DEFAULT_MULTI_TEXTCAT_MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e900815f",
      "metadata": {
        "id": "e900815f"
      },
      "source": [
        "Next, we need to provide a configuration to the <font color='blue'>TextCategorizer</font> component.\n",
        "\n",
        "We provide two parameters here, a threshold value and a model name (either Single or Multi depending on the classification task).\n",
        "\n",
        "<font color='blue'>TextCategorizer</font> internally generates a probability for each class and a class is assigned to the text if the probability of this class is higher than the threshold value.\n",
        "\n",
        "A traditional threshold value for text classification is 0.5, however, if you want to make a prediction with higher confidence, you can make the threshold higher, such as 0.6, 0.7, or 0.8.\n",
        "\n",
        "Putting it altogether, we can add a single-label <font color='blue'>TextCategorizer (tCategorizer) </font> component to the nlp pipeline as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b855803",
      "metadata": {
        "id": "2b855803"
      },
      "outputs": [],
      "source": [
        "# Import the Single Textcat Model\n",
        "# Define the model parameters: threshold and model\n",
        "from spacy.pipeline.textcat import DEFAULT_SINGLE_TEXTCAT_MODEL\n",
        "config = {\n",
        "    \"threshold\": 0.5,\n",
        "    \"model\": DEFAULT_SINGLE_TEXTCAT_MODEL\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c51fee97",
      "metadata": {
        "id": "c51fee97"
      },
      "outputs": [],
      "source": [
        "# Define the Textcat object (tCategorizer)\n",
        "tCategorizer = nlp.add_pipe(\"textcat\", config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c57a94bf",
      "metadata": {
        "id": "c57a94bf"
      },
      "source": [
        "Take a quick look on the <font color='blue'>tCategorizer</font>:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "682d5452",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "682d5452",
        "outputId": "de4466f1-08ca-47da-9b62-82bf97ead331"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.pipeline.textcat.TextCategorizer at 0x78a2c498be90>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "tCategorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1437dd34",
      "metadata": {
        "id": "1437dd34"
      },
      "source": [
        "Adding a multilabel component to the nlp pipeline is similar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd31b7fc",
      "metadata": {
        "id": "cd31b7fc"
      },
      "outputs": [],
      "source": [
        "from spacy.pipeline.textcat_multilabel import DEFAULT_MULTI_TEXTCAT_MODEL\n",
        "config = {\n",
        "    \"threshold\": 0.5,\n",
        "    \"model\": DEFAULT_MULTI_TEXTCAT_MODEL\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13d66d27",
      "metadata": {
        "id": "13d66d27"
      },
      "outputs": [],
      "source": [
        "tCategorizer = nlp.add_pipe(\"textcat_multilabel\", config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20b87d68",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20b87d68",
        "outputId": "a4b1b3df-0821-4a9f-be5c-6d251767be33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.pipeline.textcat_multilabel.MultiLabel_TextCategorizer at 0x78a1983c3e30>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "tCategorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0803394f",
      "metadata": {
        "id": "0803394f"
      },
      "source": [
        "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
        "\n",
        "In the last line of each of the preceding code blocks, we added a <font color='blue'>TextCategorizer</font> pipeline component to the nlp pipeline object.\n",
        "The newly created <font color='blue'>TextCategorizer</font> component is captured by the textcat variable.\n",
        "\n",
        "Now, We're ready to train the <font color='blue'>TextCategorizer</font> component."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2738a1bd",
      "metadata": {
        "id": "2738a1bd"
      },
      "source": [
        "### 14.4.2 Formatting training data for the TextCategorizer\n",
        "\n",
        "Let's start our code by preparing a small training set.\n",
        "\n",
        "We'll prepare a customer sentiment dataset for <font color='blue'>Binary Text Classification</font>.\n",
        "\n",
        "The label will be called <font color='blue'>sentiment</font> and can obtain two possible values, 0 and 1 corresponding to negative and positive sentiment.\n",
        "\n",
        "The following training set contains 6 examples from IMDB, 3 being positive and 3 being negative:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e084a8e5",
      "metadata": {
        "id": "e084a8e5"
      },
      "outputs": [],
      "source": [
        "movie_comment1 = [\n",
        "    (\"This movie is perfect and worth watching. \",{\"cats\": {\"Positive Sentiment\": 1}}),\n",
        "    (\"This movie is great, the performance of Al Pacino is brilliant.\", {\"cats\": {\"Positive Sentiment\": 1}}),\n",
        "    (\"A very good and funny movie. It should be the best this year!\", {\"cats\": {\"Positive Sentiment\": 1}}),\n",
        "    (\"This movie is so bad that I really want to leave after the first hour watching.\", {\"cats\": {\"Positive Sentiment\": 0}}),\n",
        "    (\"Even free I won't see this movie again. Totally failure!\", {\"cats\": {\"Positive Sentiment\": 0}}),\n",
        "    (\"I think it is the worst movie I saw so far this year.\", {\"cats\": {\"Positive Sentiment\": 0}})\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37e2d766",
      "metadata": {
        "id": "37e2d766"
      },
      "source": [
        "Check on any movie_comment1 element:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85696b84",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85696b84",
        "outputId": "971d6f87-0536-4e80-8027-466934cdf1bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('This movie is great, the performance of Al Pacino is brilliant.',\n",
              " {'cats': {'Positive Sentiment': 1}})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "movie_comment1 [1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1769594d",
      "metadata": {
        "id": "1769594d"
      },
      "source": [
        "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
        "\n",
        "- Each training example (movie_coment1) is a tuple object con-sist of a text and a nested dictionary.\n",
        "- The dictionary contains the class category in a format that spaCy recognizes.\n",
        "- The cats field means the categories.\n",
        "- Then we include the class category sentiment and its value. The value should always be a floating-point number.\n",
        "\n",
        "\n",
        "In the code, we will introduce the class label we choose to the TextCategorizer component. Let's see the complete code.\n",
        "\n",
        "First, we do the necessary imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a646374",
      "metadata": {
        "id": "5a646374"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import spacy\n",
        "from spacy.training import Example\n",
        "from spacy.pipeline.textcat import DEFAULT_SINGLE_TEXTCAT_MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c82c45f",
      "metadata": {
        "id": "3c82c45f"
      },
      "source": [
        "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
        "\n",
        "- We imported the built-in library random for shuffling our dataset.\n",
        "- We imported spacy as usual, and we imported <font color='blue'>Example</font> to prepare the training examples in spaCy format.\n",
        "- In the last line of the code block, we imported a text categorizer model.\n",
        "\n",
        "Next, we'll do the pipeline and TextCategorizer component initialization:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bffc739f",
      "metadata": {
        "id": "bffc739f"
      },
      "source": [
        "Now, we'll do some work on the newly created <font color='blue'>TextCategorizer</font> component, textcat.\n",
        "\n",
        "We'll introduce our label sentiment to the <font color='blue'>TextCategorizer</font> componenet by calling add_label.\n",
        "\n",
        "Then, we need to initialize this component with our examples.\n",
        "\n",
        "The following code adds our label to the <font color='blue'>TextCategorizer</font> component and then initializes the <font color='blue'>TextCategorizer</font> model's weights with the training examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d90ac9a",
      "metadata": {
        "id": "8d90ac9a"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import spacy\n",
        "from spacy.training import Example\n",
        "from spacy.pipeline.textcat import DEFAULT_SINGLE_TEXTCAT_MODEL\n",
        "\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "config = {\n",
        "    \"threshold\": 0.5,\n",
        "    \"model\": DEFAULT_SINGLE_TEXTCAT_MODEL\n",
        "}\n",
        "\n",
        "tCategorizer = nlp.add_pipe(\"textcat\", config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ace4e97",
      "metadata": {
        "id": "3ace4e97"
      },
      "source": [
        "Let's have a look on the <font color='blue'>pipe_names</font>:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59be1145",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59be1145",
        "outputId": "6f387fe6-4e03-4df3-9e99-13b50309cfda"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec',\n",
              " 'tagger',\n",
              " 'parser',\n",
              " 'attribute_ruler',\n",
              " 'lemmatizer',\n",
              " 'ner',\n",
              " 'textcat']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "nlp.pipe_names"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a84e9b06",
      "metadata": {
        "id": "a84e9b06"
      },
      "source": [
        "Now, we'll do some work on the newly created <font color='blue'>TextCategorizer</font> component, textcat. We'll introduce our label sentiment to the <font color='blue'>TextCategorizer</font> componenet by calling add_label. Then, we need to initialize this component with our examples."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14825278",
      "metadata": {
        "id": "14825278"
      },
      "source": [
        "The following code adds our label to the TextCategorizer component and then initializes the TextCategorizer model's weights with the training examples (movie_comment_exp)::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89857c35",
      "metadata": {
        "id": "89857c35"
      },
      "outputs": [],
      "source": [
        "tCategorizer.add_label(\"Positive Sentiment\")\n",
        "tCategorizer.add_label(\"Negative Sentiment\")\n",
        "\n",
        "movie_comment_exp = [Example.from_dict(nlp.make_doc(comments), category) for comments,category in movie_comment1]\n",
        "tCategorizer.initialize(lambda: movie_comment_exp, nlp=nlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1faa8c42",
      "metadata": {
        "id": "1faa8c42"
      },
      "source": [
        "Let's have a look on the <font color='blue'>movie_comment_exp</font>:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc8f2f6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc8f2f6b",
        "outputId": "a28b3a49-a8aa-4113-8b17-e71fa9e867d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'doc_annotation': {'cats': {'Positive Sentiment': 1}, 'entities': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['This', 'movie', 'is', 'perfect', 'and', 'worth', 'watching', '.'], 'SPACY': [True, True, True, True, True, True, False, True], 'TAG': ['', '', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6, 7], 'DEP': ['', '', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0, 0]}},\n",
              " {'doc_annotation': {'cats': {'Positive Sentiment': 1}, 'entities': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['This', 'movie', 'is', 'great', ',', 'the', 'performance', 'of', 'Al', 'Pacino', 'is', 'brilliant', '.'], 'SPACY': [True, True, True, False, True, True, True, True, True, True, True, False, False], 'TAG': ['', '', '', '', '', '', '', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], 'DEP': ['', '', '', '', '', '', '', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}},\n",
              " {'doc_annotation': {'cats': {'Positive Sentiment': 1}, 'entities': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['A', 'very', 'good', 'and', 'funny', 'movie', '.', 'It', 'should', 'be', 'the', 'best', 'this', 'year', '!'], 'SPACY': [True, True, True, True, True, False, True, True, True, True, True, True, True, False, False], 'TAG': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'DEP': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}},\n",
              " {'doc_annotation': {'cats': {'Positive Sentiment': 0}, 'entities': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['This', 'movie', 'is', 'so', 'bad', 'that', 'I', 'really', 'want', 'to', 'leave', 'after', 'the', 'first', 'hour', 'watching', '.'], 'SPACY': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False], 'TAG': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], 'DEP': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}},\n",
              " {'doc_annotation': {'cats': {'Positive Sentiment': 0}, 'entities': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['Even', 'free', 'I', 'wo', \"n't\", 'see', 'this', 'movie', 'again', '.', 'Totally', 'failure', '!'], 'SPACY': [True, True, True, False, True, True, True, True, False, True, True, False, False], 'TAG': ['', '', '', '', '', '', '', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], 'DEP': ['', '', '', '', '', '', '', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}},\n",
              " {'doc_annotation': {'cats': {'Positive Sentiment': 0}, 'entities': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['I', 'think', 'it', 'is', 'the', 'worst', 'movie', 'I', 'saw', 'so', 'far', 'this', 'year', '.'], 'SPACY': [True, True, True, True, True, True, True, True, True, True, True, True, False, False], 'TAG': ['', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], 'DEP': ['', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "movie_comment_exp"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fc2b460",
      "metadata": {
        "id": "4fc2b460"
      },
      "source": [
        "### 14.4.3 System Training\n",
        "\n",
        "We're ready to define the training loop.\n",
        "\n",
        "First of all, we'll disable other pipe components so that only textcat will be trained.\n",
        "\n",
        "Second, we will create an optimizer object by calling resume_training, keeping the weights of the existing statistical models.\n",
        "\n",
        "For each epoch, we go over training examples one by one and update the weights of textcat. We go over the data for 20 epochs.\n",
        "\n",
        "Try the whole program together with the training loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a572eac7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a572eac7",
        "outputId": "b99a0069-c27c-48eb-b7c5-a652817cb73b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('This movie is perfect and worth watching. ',\n",
              "  {'cats': {'Positive Sentiment': 1}}),\n",
              " ('This movie is great, the performance of Al Pacino is brilliant.',\n",
              "  {'cats': {'Positive Sentiment': 1}}),\n",
              " ('A very good and funny movie. It should be the best this year!',\n",
              "  {'cats': {'Positive Sentiment': 1}}),\n",
              " ('This movie is so bad that I really want to leave after the first hour watching.',\n",
              "  {'cats': {'Positive Sentiment': 0}}),\n",
              " (\"Even free I won't see this movie again. Totally failure!\",\n",
              "  {'cats': {'Positive Sentiment': 0}}),\n",
              " ('I think it is the worst movie I saw so far this year.',\n",
              "  {'cats': {'Positive Sentiment': 0}})]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "movie_comment1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fa3440a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fa3440a",
        "outputId": "b4a8b7cd-1a12-48d1-e1a6-6c9b367b6068"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch # 0 Losses:  {'textcat': 1.4792710095643997}\n",
            "Epoch # 1 Losses:  {'textcat': 2.6308142691850662}\n",
            "Epoch # 2 Losses:  {'textcat': 3.6676070149987936}\n",
            "Epoch # 3 Losses:  {'textcat': 4.674441431649029}\n",
            "Epoch # 4 Losses:  {'textcat': 5.741871039615944}\n",
            "Epoch # 5 Losses:  {'textcat': 6.843784660042729}\n",
            "Epoch # 6 Losses:  {'textcat': 7.804021488973376}\n",
            "Epoch # 7 Losses:  {'textcat': 8.826249296340393}\n",
            "Epoch # 8 Losses:  {'textcat': 9.695674010694347}\n",
            "Epoch # 9 Losses:  {'textcat': 10.726456835920544}\n",
            "Epoch # 10 Losses:  {'textcat': 11.514744136210311}\n",
            "Epoch # 11 Losses:  {'textcat': 12.517857959471485}\n",
            "Epoch # 12 Losses:  {'textcat': 13.28767119155691}\n",
            "Epoch # 13 Losses:  {'textcat': 14.255417779423624}\n",
            "Epoch # 14 Losses:  {'textcat': 15.075467424831459}\n",
            "Epoch # 15 Losses:  {'textcat': 15.946973777783}\n",
            "Epoch # 16 Losses:  {'textcat': 16.84092359114709}\n",
            "Epoch # 17 Losses:  {'textcat': 17.62122811145673}\n",
            "Epoch # 18 Losses:  {'textcat': 18.44872479542761}\n",
            "Epoch # 19 Losses:  {'textcat': 19.258456632078264}\n"
          ]
        }
      ],
      "source": [
        "# Full implementation of the Movie Sentiment Analysis System\n",
        "import random\n",
        "import spacy\n",
        "from spacy.training import Example\n",
        "from spacy.pipeline.textcat import DEFAULT_SINGLE_TEXTCAT_MODEL\n",
        "\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "config = {\n",
        "    \"threshold\": 0.5,\n",
        "    \"model\": DEFAULT_SINGLE_TEXTCAT_MODEL\n",
        "}\n",
        "\n",
        "tCategorizer = nlp.add_pipe(\"textcat\", config=config)\n",
        "\n",
        "tCategorizer.add_label(\"Positive Sentiment\")\n",
        "tCategorizer.add_label(\"Negative Sentiment\")\n",
        "\n",
        "movie_comment_exp = [Example.from_dict(nlp.make_doc(comments), category) for comments,category in movie_comment1]\n",
        "tCategorizer.initialize(lambda: movie_comment_exp, nlp=nlp)\n",
        "\n",
        "epochs=20\n",
        "\n",
        "losses = {}\n",
        "\n",
        "with nlp.select_pipes(enable=\"textcat\"):\n",
        "    optimizer = nlp.resume_training()\n",
        "    for i in range(epochs):\n",
        "        random.shuffle(movie_comment1)\n",
        "        for comments, category in movie_comment1:\n",
        "            mdoc = nlp.make_doc(comments)\n",
        "            exp = Example.from_dict(mdoc, category)\n",
        "            nlp.update([exp], sgd=optimizer, losses=losses)\n",
        "        print(\"Epoch #\",i, \"Losses: \",losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fb3a4a1",
      "metadata": {
        "id": "8fb3a4a1"
      },
      "source": [
        "### 14.4.4 System Testing\n",
        "\n",
        "Let's test the new text categorizer component.\n",
        "The <font color='blue'>doc.cats</font> property holds the class labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72268758",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72268758",
        "outputId": "f8700dca-330d-4d7e-e6d4-391494f591b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Positive Sentiment': 0.9283635020256042,\n",
              " 'Negative Sentiment': 0.07163646817207336}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Test 1: This movie sucks\n",
        "test1 = nlp(\"This movie sucks and the worst I ever saw.\")\n",
        "test1.cats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cd5337a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cd5337a",
        "outputId": "5f53a1a4-2fe3-46fb-9931-8320377998af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Positive Sentiment': 0.9060717821121216,\n",
              " 'Negative Sentiment': 0.0939282551407814}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Test 2: I'll watch it again, how amazing.\n",
        "test2 = nlp(\"This movie really very great!\")\n",
        "test2.cats"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04b2c110",
      "metadata": {
        "id": "04b2c110"
      },
      "source": [
        "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
        "\n",
        "Great! <br>\n",
        "Our small dataset successfully trained the spaCy text classifier for a binary text classification problem to perform correct sentiment analysis nicely. Now, we'll see how to perform multilabel classification."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31196412",
      "metadata": {
        "id": "31196412"
      },
      "source": [
        "### 14.4.5 Training TextCategorizer for multilabel classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4735fd1",
      "metadata": {
        "id": "c4735fd1"
      },
      "source": [
        "Recall from the first section that multilabel classification means the classifier can predict more than one label for an example text. Naturally, the classes are not mutually exclusive at all.\n",
        "\n",
        "In order to train a multilabel classifier, we need to provide a dataset that contains examples that have more than one label.\n",
        "\n",
        "To train spaCy's TextCategorizer for multilabel classification, we'll again start by building a small training set.\n",
        "\n",
        "This time, we'll form a set of movie reviews, where the labels are:\n",
        "- ACTION\n",
        "- SCIFI\n",
        "- WEEKEND\n",
        "\n",
        "Here is our small dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "232dd5e2",
      "metadata": {
        "id": "232dd5e2"
      },
      "outputs": [],
      "source": [
        "movie_comment2 = [\n",
        "    (\"This movie is great for weekend watching.\", {\"cats\": {\"WEEKEND\": True}}),\n",
        "    (\"This a 100% action movie, I enjoy it.\", {\"cats\": {\"ACTION\": True}}),\n",
        "    (\"Avatar is the best Scifi movie I ever seen!\"  , {\"cats\": {\"SCIFI\": True}}),\n",
        "    (\"Such a good Scifi movie to watch during the weekend!\", {\"cats\": {\"WEEKEND\": True, \"SCIFI\":True}}),\n",
        "    (\"Matrix a great Scifi movie with a lot of action. Pure action, great!\", {\"cats\": {\"SCIFI\": True, \"ACTION\": True}})\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9221351",
      "metadata": {
        "id": "e9221351"
      },
      "source": [
        "Check the data set first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20f50a5c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20f50a5c",
        "outputId": "0e9b68d9-d828-402d-ac87-b0efa5ef3cc5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('This movie is great for weekend watching.', {'cats': {'WEEKEND': True}}),\n",
              " ('This a 100% action movie, I enjoy it.', {'cats': {'ACTION': True}}),\n",
              " ('Avatar is the best Scifi movie I ever seen!', {'cats': {'SCIFI': True}}),\n",
              " ('Such a good Scifi movie to watch during the weekend!',\n",
              "  {'cats': {'WEEKEND': True, 'SCIFI': True}}),\n",
              " ('Matrix a great Scifi movie with a lot of action. Pure action, great!',\n",
              "  {'cats': {'SCIFI': True, 'ACTION': True}})]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "movie_comment2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b5d00ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b5d00ae",
        "outputId": "58123370-f128-4a26-c78a-29a033f04939"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('This a 100% action movie, I enjoy it.', {'cats': {'ACTION': True}})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "movie_comment2[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5574eb68",
      "metadata": {
        "id": "5574eb68"
      },
      "source": [
        "In the dataset, we provided some examples with one label, such as the first ex-ample (the first sentence of movie_comment2, the second line of the preceding code block), and we also provided examples with more than one label, such as the fourth example of the movie_comment2.\n",
        "\n",
        "We'll make the imports after we've formed the training set:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47074edc",
      "metadata": {
        "id": "47074edc"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import spacy\n",
        "from spacy.training import Example\n",
        "from spacy.pipeline.textcat_multilabel import DEFAULT_MULTI_TEXTCAT_MODEL\n",
        "\n",
        "nlp = spacy.load('en_core_web_md')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34805a63",
      "metadata": {
        "id": "34805a63"
      },
      "source": [
        "Here, the last line is different than the code of the previous section. We imported the multilabel model instead of the single-label model.\n",
        "\n",
        "Next, we add the multilabel classifier component to the nlp pipeline.\n",
        "\n",
        "Again, pay attention to the pipeline component name – this time, it is <font color='blue'>textcat_multilabel</font>, compared to the previous section's <font color='blue'>textcat</font>:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18966edc",
      "metadata": {
        "id": "18966edc"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"threshold\": 0.5,\n",
        "    \"model\": DEFAULT_MULTI_TEXTCAT_MODEL\n",
        "}\n",
        "tCategorizer = nlp.add_pipe(\"textcat_multilabel\", config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78bc99f9",
      "metadata": {
        "id": "78bc99f9"
      },
      "source": [
        "Adding the labels to the <font color='blue'>TextCategorizer</font> component and initializing the model is similar to the Training the spaCy text classifier section.\n",
        "\n",
        "This time, we'll add three labels instead of one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1325e8c0",
      "metadata": {
        "id": "1325e8c0"
      },
      "outputs": [],
      "source": [
        "categories = [\"SCIFI\", \"ACTION\", \"WEEKEND\"]\n",
        "\n",
        "for category in categories:\n",
        "    tCategorizer.add_label(category)\n",
        "\n",
        "movie_comment_exp = [Example.from_dict(nlp.make_doc(comments), category) for comments,category in movie_comment2]\n",
        "tCategorizer.initialize(lambda: movie_comment_exp, nlp=nlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "925e05b1",
      "metadata": {
        "id": "925e05b1"
      },
      "source": [
        "We're ready to define the training loop. The code functions are similar to the previous section's code. The only difference is the component name in the first line.\n",
        "\n",
        "Now it's textcat_multilabel:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe35c9d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe35c9d1",
        "outputId": "c0260b62-7d0c-4143-f90c-7d52734ddc0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'textcat_multilabel': 0.6650418508797884}\n",
            "{'textcat_multilabel': 0.6729632093059597}\n",
            "{'textcat_multilabel': 0.6729651333996843}\n",
            "{'textcat_multilabel': 0.6729657640557907}\n",
            "{'textcat_multilabel': 0.6729661702707842}\n",
            "{'textcat_multilabel': 0.6729665031395103}\n",
            "{'textcat_multilabel': 0.6729667914336264}\n",
            "{'textcat_multilabel': 0.672967066478946}\n",
            "{'textcat_multilabel': 0.672967327170535}\n",
            "{'textcat_multilabel': 0.6729675735745946}\n",
            "{'textcat_multilabel': 0.6729678142438439}\n",
            "{'textcat_multilabel': 0.672968046616668}\n",
            "{'textcat_multilabel': 0.6729682733416721}\n",
            "{'textcat_multilabel': 0.6729684939538395}\n",
            "{'textcat_multilabel': 0.6729687098038284}\n",
            "{'textcat_multilabel': 0.6729689231260405}\n",
            "{'textcat_multilabel': 0.6729691308368311}\n",
            "{'textcat_multilabel': 0.6729693356985442}\n",
            "{'textcat_multilabel': 0.6729695355037795}\n",
            "{'textcat_multilabel': 0.6729697320112674}\n"
          ]
        }
      ],
      "source": [
        "epochs=20\n",
        "losses = {}\n",
        "\n",
        "with nlp.select_pipes(enable=\"textcat_multilabel\"):\n",
        "    optimizer = nlp.resume_training()\n",
        "    for i in range(epochs):\n",
        "        random.shuffle(movie_comment2)\n",
        "        for comments, category in movie_comment2:\n",
        "            mdoc = nlp.make_doc(comments)\n",
        "            exp = Example.from_dict(mdoc, category)\n",
        "            nlp.update([exp], sgd=optimizer, losses=losses)\n",
        "        print(losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c514641",
      "metadata": {
        "id": "8c514641"
      },
      "source": [
        "The output should look similar to the output of the previous section, a loss value per epoch.\n",
        "\n",
        "Now, let's test our brand new multilabel classifier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fe54ca9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fe54ca9",
        "outputId": "f01e99b8-e96f-480d-a705-51f308dd4a6a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'SCIFI': 0.9843090772628784,\n",
              " 'ACTION': 0.9803446531295776,\n",
              " 'WEEKEND': 0.930459201335907}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "test3 = nlp(\"Definitely in my weekend scifi movie night list\")\n",
        "test3.cats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61289513",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61289513",
        "outputId": "d1a5b34c-c322-4214-b9a8-e376fcf05ce6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'SCIFI': 0.9560961723327637,\n",
              " 'ACTION': 0.8947148323059082,\n",
              " 'WEEKEND': 0.9996786117553711}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "test4 = nlp(\"Go to watch action scifi movie this weekend.\")\n",
        "test4.cats"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c6961ca",
      "metadata": {
        "id": "3c6961ca"
      },
      "source": [
        "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
        "\n",
        "Notice that although the sample size is very small, the multiple text categorizer can still correctly classify the two IMDB user comments into the three categories: SCIFI, ACTION and WEEKEND. In real world situation, we need over thousands of IMDB user comments to perform a better and more stable sentiment analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94f41e87",
      "metadata": {
        "id": "94f41e87"
      },
      "source": [
        "In this section, we've learned how to train spaCy's TextCategorizer component for binary text classification and multilabel text classification.\n",
        "\n",
        "Now, we'll train TextCategorizer on a real-world dataset for a sentiment analysis problem using IMDB user comment dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cfd5832",
      "metadata": {
        "id": "2cfd5832"
      },
      "source": [
        "<img src=\"./workshop.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
        "\n",
        "### Lab Task 1  Movie comments from IMDB.com\n",
        "\n",
        "One of the most important User Comments classfication used in social media is the classification of movie whether it is: Good, Average or Bad.\n",
        "\n",
        "In the workshop, you try to build a simple Movie Comment Classification with the user comments from IMDB.com, the world biggest movie social media with millions of user comments, both positive and negative.\n",
        "\n",
        "1. To train your system, try to collect 900 comments with 300 Good, 300 Average and 300 Bad comments. Be sure to be make sense. Otherwise your system doesn't work.\n",
        "2. Build a Multilabel Classification System to create the THREE movie comments: Good, Average or Bad.\n",
        "3. Train your system for at least 100 epochs.\n",
        "4. Use 10 examples test for it. See whether it works."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "322af00c",
      "metadata": {
        "id": "322af00c"
      },
      "source": [
        "## 14.5 Pipeline Sentiment Analysis with spaCy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a5510ba",
      "metadata": {
        "id": "6a5510ba"
      },
      "source": [
        "### 14.5.1 IMDB Large Movie Review Dataset\n",
        "\n",
        "In this section, we'll work on a real-world dataset and train spaCy's TextCategorizer on this dataset.\n",
        "\n",
        "In this section, we use the IMDB Large Movie Review Dataset from Kaggle (https://www.kaggle.com/code/nisargchodavadiya/movie-review-analytics-sentiment-ratings/data).\n",
        "\n",
        "The original dataset imdb_sup.csv is huge, with 50,000 rows. To speed up the training procedure, we down-size the dataset to select the first 5000 records into datafile imdb_5000.csv. This movie review dataset consists of: Movie reviews, review size, IMDB Rating (1-10) and Sentiment Rating (0 or 1).\n",
        "\n",
        "You can download the dataset from our workshop directory, namely: imdb_sup.csv (complete dataset) or imdb_5000.csv (5000 records)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "144a2391",
      "metadata": {
        "id": "144a2391"
      },
      "source": [
        "### 14.5.2 Explore the dataset\n",
        "\n",
        "Begin we start the Sentiment Analysis, let's get some basic idea on what dataset we have.\n",
        "\n",
        "1. First, we'll do the imports for reading and visualizing the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07fed5bb",
      "metadata": {
        "id": "07fed5bb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "705eb2fb",
      "metadata": {
        "id": "705eb2fb"
      },
      "source": [
        "2. We'll read the imdb_5000.csv datafile into a pandas DataFrame (mcom-mentDF) and output the shape of the DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mcommentDF = pd.read_csv('/content/sample_data/imdb_5000.csv', encoding='ISO-8859-1')\n"
      ],
      "metadata": {
        "id": "Lxyxf4-gU4Ko"
      },
      "id": "Lxyxf4-gU4Ko",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0faaf11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "d0faaf11",
        "outputId": "f1ed5d0f-05e3-4e24-c6bf-1fd07a42f70f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "'utf-8' codec can't decode byte 0x80 in position 231342: invalid start byte",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-360880935427>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmcommentDF\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/sample_data/imdb_5000.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1897\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m# Fail here loudly instead of in cython after reading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyarrow\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 231342: invalid start byte"
          ]
        }
      ],
      "source": [
        "mcommentDF=pd.read_csv('/content/sample_data/imdb_5000.csv')"
      ]
    },
    {
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Try reading the CSV with a different encoding\n",
        "try:\n",
        "    mcommentDF = pd.read_csv('/content/sample_data/imdb_5000.csv', encoding='latin1')\n",
        "except UnicodeDecodeError:\n",
        "    # If latin1 fails, try another common encoding like ISO-8859-1\n",
        "    mcommentDF = pd.read_csv('/content/sample_data/imdb_5000.csv', encoding='ISO-8859-1')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "1NHNgttVWRQg"
      },
      "id": "1NHNgttVWRQg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a029ab8c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a029ab8c",
        "outputId": "de48b10d-c11e-408b-b685-0fa99719b37c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "mcommentDF.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e13a815",
      "metadata": {
        "id": "6e13a815"
      },
      "source": [
        "Note: This IMDB movie comment dataset contains 5000 records, each record with 3 fields of attributes: Review, Rating and Sentiment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78d68637",
      "metadata": {
        "id": "78d68637"
      },
      "source": [
        "3. Next, we examine the rows and the columns of the dataset by printing the first few rows using the head() method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f63fd0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0f63fd0a",
        "outputId": "59a62d7a-5885-425e-fc68-221f426adbf0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           ï»¿Review  Rating  Sentiment\n",
              "0                              **Possible Spoilers**       1          0\n",
              "1                   Read the book, forget the movie!       2          0\n",
              "2                        **Possible Spoilers Ahead**       2          0\n",
              "3          What a script, what a story, what a mess!       2          0\n",
              "4  I hope this group of film-makers never re-unites.       1          0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-56d6a49c-a2d1-4ca5-9305-873857603f08\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ï»¿Review</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>**Possible Spoilers**</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Read the book, forget the movie!</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>**Possible Spoilers Ahead**</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What a script, what a story, what a mess!</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I hope this group of film-makers never re-unites.</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-56d6a49c-a2d1-4ca5-9305-873857603f08')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-56d6a49c-a2d1-4ca5-9305-873857603f08 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-56d6a49c-a2d1-4ca5-9305-873857603f08');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-39e2be67-f6d7-4d0c-ac67-aefddf7cb376\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-39e2be67-f6d7-4d0c-ac67-aefddf7cb376')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-39e2be67-f6d7-4d0c-ac67-aefddf7cb376 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "mcommentDF",
              "summary": "{\n  \"name\": \"mcommentDF\",\n  \"rows\": 5000,\n  \"fields\": [\n    {\n      \"column\": \"\\u00ef\\u00bb\\u00bfReview\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4951,\n        \"samples\": [\n          \"I'm still laughing- Not! I'm still asking my myself what the point was. I barely got a chuckle, this movie sucks. It tries to be charming and touching, but it turns out stupid. I do not recommend it.\",\n          \"The greatest sin in life is being dull, and this movie is crashingly boring. its funny, its left out of his \\\"a life in film\\\" documentary. He goes from a long piece on \\\"Stardust Memories\\\" and then fast forwards to \\\"Zelig\\\". This little piece of cubic zirconia just isn't worth the effort.\",\n          \"I really enjoyed this movie. It challenged my emotions and beliefs, making it a true piece of artwork in my book. The acting was unsurpassed. I would never watch this movie with anyone I could not cry around, I don't think I cry harder to any movies, maybe because it makes me look at myself, I dunno. It is a must see.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 10,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          2,\n          9,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "mcommentDF.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a93f4914",
      "metadata": {
        "id": "a93f4914"
      },
      "source": [
        "4. As we only use the Review and Sentiment columns in this workshop; hence, we'll drop the other columns that we won't use. We’ll also call the dropna() method to drop the rows with missing values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d711c1f",
      "metadata": {
        "id": "9d711c1f"
      },
      "outputs": [],
      "source": [
        "mcommentDF_clean = mcommentDF[['Review','Sentiment']].dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9c7b9c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "c9c7b9c5",
        "outputId": "516dc456-6843-4ada-be30-a392b0a2d10c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              Review  Sentiment\n",
              "0  Kurt Russell's chameleon-like performance, cou...          1\n",
              "1  It was extremely low budget(it some scenes it ...          1\n",
              "2  James Cagney is best known for his tough chara...          1\n",
              "3  Following the brilliant \"Goy??kiba\" (aka. \"Han...          1\n",
              "4  One of the last classics of the French New Wav...          1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1ab3a80e-bf8f-4c38-925d-c310a5502d32\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Kurt Russell's chameleon-like performance, cou...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It was extremely low budget(it some scenes it ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>James Cagney is best known for his tough chara...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Following the brilliant \"Goy??kiba\" (aka. \"Han...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>One of the last classics of the French New Wav...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1ab3a80e-bf8f-4c38-925d-c310a5502d32')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1ab3a80e-bf8f-4c38-925d-c310a5502d32 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1ab3a80e-bf8f-4c38-925d-c310a5502d32');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-4b81b56c-d035-4155-a03a-574fa075da23\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4b81b56c-d035-4155-a03a-574fa075da23')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-4b81b56c-d035-4155-a03a-574fa075da23 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "mcommentDF_clean",
              "summary": "{\n  \"name\": \"mcommentDF_clean\",\n  \"rows\": 5849,\n  \"fields\": [\n    {\n      \"column\": \"Review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5840,\n        \"samples\": [\n          \"So you think a talking parrot is not your cup of tea huh? Well, think again. Paulie is a wonderful film filled with touching moments.The characters are all lovable especially Paulie as he enters the lives of many people on his journey.It is journey worth experiencing. Don't miss it! It is available on home video.\",\n          \"This is perhaps the best rockumentary ever- a British, better This Is Spinal Tap. The characters are believable, the plot is great, and you can genuinely empathise with some of the events- such as Ray's problem with fitting in the band.The soundtrack is excellent. Real period stuff, even if it is in the same key, you'll be humming some of the songs for days. What I liked was the nearly all-British cast, with some of the favourite household names. Ray's wife is priceless...The film never drags, it just goes at the right pace, and has some genuinely funny sections in it. A generator of some really good catchphrases!It's a hidden diamond.\",\n          \"After witnessing his wife (Linda Hoffman) engaging in sexual acts with the pool boy, the already somewhat unstable dentist Dr. Feinstone (Corbin Bernsen) completely snaps which means deep trouble for his patients.This delightful semi-original and entertaining horror flick from director Brian Yuzna was a welcome change of pace from the usual horror twaddle that was passed out in the late Nineties. Although ??The Dentist' is intended to be a cheesy, fun little film, Yuzna ensures that the movie delivers the shocks and thrills that many more serious movies attempt to dispense. Despite suffering somewhat from the lack of background on the central characters, and thus allowing events that should have been built up to take place over a couple of days, the movie is intriguing, generally well scripted and well paced which allows the viewer to maintain interest, even during the more ludicrous of moments. ??The Dentist' suffers, on occasion, from dragging but unlike the much inferior 1998 sequel, there are only sporadic uninteresting moments, and in general the movie follows itself nicely.Corbin Bernsen was very convincing in the role of the sadistic, deranged and perfectionist Dr. Alan Feinstone. The way Bernsen is able to credibly recite his lines, especially with regards to the foulness and immorality of sex (particularly fellatio), is something short of marvellous. While many actors may have trouble portraying a cleanliness obsessed psycho without it coming off as too cheesy or ridiculous, Bernsen seems to truly fit the personality of the character he attempts to portray and thus makes the film all that more enjoyable. Had ??The Dentist' not been intended to be a fun, almost comical, horror movie, Bernsen's performance would probably have been much more powerful. Sadly, the rest of the cast (including a pre-fame Mark Ruffalo) failed to put in very good performances and although the movie was not really damaged by this, stronger performances could have added more credibility to the flick.??The Dentist' is not a horror film that is meant to be taken seriously but is certainly enjoyable, particularly (I would presume) for fans of cheesy horror. Those who became annoyed at the number of ??Scream' (1996) clones from the late Nineties may very well find this a refreshing change, as I did. A seldom dull and generally well paced script as well as some proficient direction helps to make ??The Dentist' one of the more pleasurable cheesy horrors from the 1990's. On top of this we are presented with some particularly grizly and (on the whole) realistic scenes of dental torture, which should keep most gorehounds happy. Far from perfect but far from bad as well, ??The Dentist' is a flick that is easily worth watching at least once. My rating for ??The Dentist' ?? 6.5/10.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "mcommentDF_clean.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8cccc31",
      "metadata": {
        "id": "d8cccc31"
      },
      "source": [
        "5. We can have a quick look at how the review scores are distributed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb7c40d5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "cb7c40d5",
        "outputId": "ea989796-13d1-49d2-d61c-c7b6a794d1fd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "axplot=mcommentDF.Rating.value_counts().plot(kind='bar', colormap='Paired')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "404c6d2a",
      "metadata": {
        "id": "404c6d2a"
      },
      "source": [
        "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
        "\n",
        "1.\tAs shown in the statistics, users are more preferrable to give high rating (ie. 8 or above), and the rating 10 is the highest rank.\n",
        "2.\tSo, in order to have balance in the sample data rating, it is bet-ter to choose the sample set that with more even distribution.\n",
        "3.\tOf course, we can check the system performance first. If it is not as good as predicted, we can fine-tune the sampling method to enhance the system performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "250f6881",
      "metadata": {
        "id": "250f6881"
      },
      "source": [
        "Here we’ll use the sentiment which has been already labeled."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8019e00",
      "metadata": {
        "id": "e8019e00"
      },
      "source": [
        "6. Let's plot the distribution of the ratings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a4850ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "6a4850ca",
        "outputId": "5be3cd68-6d1c-4d66-bb2c-88bf4abff29a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "axplot=mcommentDF.Sentiment.value_counts().plot(kind='bar', colormap='Paired')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c307424",
      "metadata": {
        "id": "4c307424"
      },
      "source": [
        "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
        "\n",
        "Note that the resulting rating distribution looks much better than the previous one. Still, the number of positive reviews is greater, but the number of negative reviews is significant as well, as can be seen from this plot."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a6d3385",
      "metadata": {
        "id": "3a6d3385"
      },
      "source": [
        "After processing the dataset, we reduced it to a two-column dataset with nega-tive and positive ratings. We call mcommentDF.head() once again and the follow-ing is the result we get:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7f4befd",
      "metadata": {
        "id": "a7f4befd",
        "outputId": "5db3b5a1-e4d1-4d9b-c29b-a62d826a1aa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              Review  Rating  Sentiment\n",
              "0  Kurt Russell's chameleon-like performance, cou...      10          1\n",
              "1  It was extremely low budget(it some scenes it ...       8          1\n",
              "2  James Cagney is best known for his tough chara...       8          1\n",
              "3  Following the brilliant \"Goy??kiba\" (aka. \"Han...       8          1\n",
              "4  One of the last classics of the French New Wav...      10          1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1ded7fd1-1f33-4229-9fed-d23d1d62ed80\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Kurt Russell's chameleon-like performance, cou...</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It was extremely low budget(it some scenes it ...</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>James Cagney is best known for his tough chara...</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Following the brilliant \"Goy??kiba\" (aka. \"Han...</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>One of the last classics of the French New Wav...</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1ded7fd1-1f33-4229-9fed-d23d1d62ed80')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1ded7fd1-1f33-4229-9fed-d23d1d62ed80 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1ded7fd1-1f33-4229-9fed-d23d1d62ed80');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-9d291902-cc1b-42ce-9b71-aee10225f013\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9d291902-cc1b-42ce-9b71-aee10225f013')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-9d291902-cc1b-42ce-9b71-aee10225f013 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "mcommentDF",
              "summary": "{\n  \"name\": \"mcommentDF\",\n  \"rows\": 5849,\n  \"fields\": [\n    {\n      \"column\": \"Review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5840,\n        \"samples\": [\n          \"So you think a talking parrot is not your cup of tea huh? Well, think again. Paulie is a wonderful film filled with touching moments.The characters are all lovable especially Paulie as he enters the lives of many people on his journey.It is journey worth experiencing. Don't miss it! It is available on home video.\",\n          \"This is perhaps the best rockumentary ever- a British, better This Is Spinal Tap. The characters are believable, the plot is great, and you can genuinely empathise with some of the events- such as Ray's problem with fitting in the band.The soundtrack is excellent. Real period stuff, even if it is in the same key, you'll be humming some of the songs for days. What I liked was the nearly all-British cast, with some of the favourite household names. Ray's wife is priceless...The film never drags, it just goes at the right pace, and has some genuinely funny sections in it. A generator of some really good catchphrases!It's a hidden diamond.\",\n          \"After witnessing his wife (Linda Hoffman) engaging in sexual acts with the pool boy, the already somewhat unstable dentist Dr. Feinstone (Corbin Bernsen) completely snaps which means deep trouble for his patients.This delightful semi-original and entertaining horror flick from director Brian Yuzna was a welcome change of pace from the usual horror twaddle that was passed out in the late Nineties. Although ??The Dentist' is intended to be a cheesy, fun little film, Yuzna ensures that the movie delivers the shocks and thrills that many more serious movies attempt to dispense. Despite suffering somewhat from the lack of background on the central characters, and thus allowing events that should have been built up to take place over a couple of days, the movie is intriguing, generally well scripted and well paced which allows the viewer to maintain interest, even during the more ludicrous of moments. ??The Dentist' suffers, on occasion, from dragging but unlike the much inferior 1998 sequel, there are only sporadic uninteresting moments, and in general the movie follows itself nicely.Corbin Bernsen was very convincing in the role of the sadistic, deranged and perfectionist Dr. Alan Feinstone. The way Bernsen is able to credibly recite his lines, especially with regards to the foulness and immorality of sex (particularly fellatio), is something short of marvellous. While many actors may have trouble portraying a cleanliness obsessed psycho without it coming off as too cheesy or ridiculous, Bernsen seems to truly fit the personality of the character he attempts to portray and thus makes the film all that more enjoyable. Had ??The Dentist' not been intended to be a fun, almost comical, horror movie, Bernsen's performance would probably have been much more powerful. Sadly, the rest of the cast (including a pre-fame Mark Ruffalo) failed to put in very good performances and although the movie was not really damaged by this, stronger performances could have added more credibility to the flick.??The Dentist' is not a horror film that is meant to be taken seriously but is certainly enjoyable, particularly (I would presume) for fans of cheesy horror. Those who became annoyed at the number of ??Scream' (1996) clones from the late Nineties may very well find this a refreshing change, as I did. A seldom dull and generally well paced script as well as some proficient direction helps to make ??The Dentist' one of the more pleasurable cheesy horrors from the 1990's. On top of this we are presented with some particularly grizly and (on the whole) realistic scenes of dental torture, which should keep most gorehounds happy. Far from perfect but far from bad as well, ??The Dentist' is a flick that is easily worth watching at least once. My rating for ??The Dentist' ?? 6.5/10.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 7,\n        \"max\": 10,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          8,\n          9,\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "mcommentDF.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "999b3c2f",
      "metadata": {
        "id": "999b3c2f"
      },
      "source": [
        "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
        "\n",
        "We'll finish our dataset exploration here. We saw the distribution of the review scores and the class categories. The dataset is now ready to be processed. We dropped the unused columns and converted review scores to binary class labels. Let's go ahead and start the training procedure!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61f4897a",
      "metadata": {
        "id": "61f4897a"
      },
      "source": [
        "### 14.5.3 Training the TextClassfier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1e9c040",
      "metadata": {
        "id": "d1e9c040"
      },
      "source": [
        "Now, we're ready to start the training procedure. We'll train a binary text classifier with the multilabel classifier this time. Again, let's go step by step:\n",
        "\n",
        "1. We start by importing the spaCy classes as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aee929a4",
      "metadata": {
        "id": "aee929a4"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import random\n",
        "from spacy.training import Example\n",
        "from spacy.pipeline.textcat_multilabel import DEFAULT_MULTI_TEXTCAT_MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d403eecc",
      "metadata": {
        "id": "d403eecc"
      },
      "source": [
        "2. Next, we'll create a pipeline object, <font color='blue'>nlp</font>, define the classifier configuration, and add the <font color='blue'>TextCategorizer</font> component to <font color='blue'>nlp</font> with the following configuration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8668846",
      "metadata": {
        "id": "f8668846"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "config = {\n",
        "    \"threshold\": 0.5,\n",
        "    \"model\": DEFAULT_MULTI_TEXTCAT_MODEL\n",
        "}\n",
        "\n",
        "tCategorizer = nlp.add_pipe(\"textcat_multilabel\", config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5d12ab6",
      "metadata": {
        "id": "c5d12ab6"
      },
      "source": [
        "3. After the creation of the TextCategorizer object, we will create the movie comment sample object as a list and load all the user comments and the categories into it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4e01a9b",
      "metadata": {
        "id": "b4e01a9b"
      },
      "outputs": [],
      "source": [
        "movie_comment_exp = []\n",
        "\n",
        "for idx, rw in mcommentDF.iterrows():\n",
        "    comments = rw[\"Review\"]\n",
        "    rating = rw[\"Sentiment\"]\n",
        "    category = {\"POS\": True, \"NEG\": False} if rating == 1 else {\"NEG\": True, \"POS\": False}\n",
        "    movie_comment_exp.append(Example.from_dict(nlp.make_doc(comments), {\"cats\": category}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e6c1c5c",
      "metadata": {
        "id": "3e6c1c5c"
      },
      "source": [
        "4. Let's check the <font color='blue'>movie_comment_exp</font>:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e82c400f",
      "metadata": {
        "id": "e82c400f",
        "outputId": "4fc2d0ed-ed8a-466a-91e5-65811ccd6f0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'doc_annotation': {'cats': {'POS': True, 'NEG': False}, 'entities': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['Kurt', 'Russell', \"'s\", 'chameleon', '-', 'like', 'performance', ',', 'coupled', 'with', 'John', 'Carpenter', \"'s\", 'flawless', 'filmmaking', ',', 'makes', 'this', 'one', ',', 'without', 'a', 'doubt', ',', 'one', 'of', 'the', 'finest', 'boob', '-', 'tube', 'bios', 'ever', 'aired', '.', 'It', 'holds', 'up', ',', 'too', ':', 'the', 'emotional', 'foundation', 'is', 'strong', 'enough', 'that', 'it', \"'ll\", 'never', 'age', ';', 'Carpenter', 'has', 'preserved', 'for', 'posterity', 'the', 'power', 'and', 'ultimate', 'poignancy', 'of', 'the', 'life', 'of', 'the', 'one', 'and', 'only', 'King', 'of', 'Rock', 'and', 'Roll', '.', '(', 'I', \"'d\", 'been', 'a', 'borderline', 'Elvis', 'fan', 'most', 'of', 'my', 'life', ',', 'but', 'it', 'was', \"n't\", 'until', 'I', 'saw', 'this', 'mind', '-', 'blowingly', 'moving', 'movie', 'that', 'I', 'looked', 'BEYOND', 'the', 'image', 'at', 'the', 'man', 'himself', '.', 'It', 'was', 'quite', 'a', 'revelation', '.', ')', 'ELVIS', 'remains', 'one', 'of', 'the', 'top', 'ten', 'made', '-', 'for', '-', 'tv', 'movies', 'of', 'all', 'time', '.'], 'SPACY': [True, False, True, False, False, True, False, True, True, True, True, False, True, True, False, True, True, True, False, True, True, True, False, True, True, True, True, True, False, False, True, True, True, False, True, True, True, False, True, False, True, True, True, True, True, True, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, False, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, False, False, False, False, True, True, True, True, False, False], 'TAG': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137], 'DEP': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "movie_comment_exp[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f6ece7f",
      "metadata": {
        "id": "3f6ece7f"
      },
      "source": [
        "5. We'll use <font color='blue'>POS</font> and <font color='blue'>NEG</font> labels for positive and negative sentiment, respectively. We'll introduce these labels to the new component and also initialize the component with examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3975ed7",
      "metadata": {
        "id": "f3975ed7"
      },
      "outputs": [],
      "source": [
        "tCategorizer.add_label(\"POS\")\n",
        "tCategorizer.add_label(\"NEG\")\n",
        "tCategorizer.initialize(lambda: movie_comment_exp, nlp=nlp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c929a146",
      "metadata": {
        "id": "c929a146",
        "outputId": "66fcc48b-73f8-491a-e701-9f001e1078c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<spacy.pipeline.textcat_multilabel.MultiLabel_TextCategorizer at 0x2759c267dc0>"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tCategorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de54240e",
      "metadata": {
        "id": "de54240e"
      },
      "source": [
        "6. We're ready to define the training loop! We went over the training set for two epochs, but you can go over more if you like. The following code snippet will train the new text categorizer component:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6a26c71",
      "metadata": {
        "id": "d6a26c71"
      },
      "outputs": [],
      "source": [
        "epochs = 2\n",
        "\n",
        "with nlp.select_pipes(enable=\"textcat_multilabel\"):\n",
        "    optimizer = nlp.resume_training()\n",
        "    for i in range(epochs):\n",
        "        random.shuffle(movie_comment_exp)\n",
        "        for exp in movie_comment_exp:\n",
        "            nlp.update([exp], sgd=optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef8fdd82",
      "metadata": {
        "id": "ef8fdd82"
      },
      "source": [
        "7. Finally, we'll test how the text classifier component works for two example sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a83c2981",
      "metadata": {
        "id": "a83c2981"
      },
      "outputs": [],
      "source": [
        "test5 = nlp(\"This is the best movie that I have ever watched\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "389143c3",
      "metadata": {
        "id": "389143c3"
      },
      "outputs": [],
      "source": [
        "test5.cats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f232cfd",
      "metadata": {
        "id": "4f232cfd"
      },
      "outputs": [],
      "source": [
        "test6 = nlp(\"This movie is so bad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20469d5c",
      "metadata": {
        "id": "20469d5c"
      },
      "outputs": [],
      "source": [
        "test6.cats"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1af76b55",
      "metadata": {
        "id": "1af76b55"
      },
      "source": [
        "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
        "\n",
        "Noted that both the <font color='blue'>NEG</font> and <font color='blue'>POS</font> labels appear in the prediction result because we used the multilabel classifier. The results look good. The first sentence outputs a very high positive probability, and the second sentence is predicted as negative with a high probability."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad687be8",
      "metadata": {
        "id": "ad687be8"
      },
      "source": [
        "We've completed training spaCy's text classifier component.\n",
        "\n",
        "In the next section, we'll dive into the world of a very popular deep learning library, Keras.\n",
        "\n",
        "We'll explore how to write Keras code to do text classification by using another popular machine learning library – TensorFlow's Keras API.\n",
        "\n",
        "Let's go ahead and explore Keras and TensorFlow!  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba97fec5",
      "metadata": {
        "id": "ba97fec5"
      },
      "source": [
        "## 14.6. Artificial Neural Network in a Nutshell\n",
        "\n",
        "In this last workshop section, we will learn how to incorporate spaCy technology with ANN (Artificial Neural Networks) technology by using TensorFlow and its Keras package.\n",
        "\n",
        "Before that, let’s have a brief overview of a typical ANN.\n",
        "A typical ANN consists of three layers:\n",
        "1.\tInput layer – consists of input neurons (nodes)\n",
        "2.\tHidden layer – consists of hidden neurons (nodes)\n",
        "3.\tOutput layer – consists of output neurons (nodes)\n",
        "\n",
        "Through the provision of sufficient sample inputs and target outputs pairs of network training, the ANN will learn the knowledge by the update of its network weights.\n",
        "\n",
        "After sufficient network training to a certain accuracy, the network can be use to predict (match) unseen inputs to the corresponding output result.\n",
        "\n",
        "Fig. 14.4 shows a typical ANN architecture.\n",
        "\n",
        "<img src=\"./Fig 14.4.jpg\" width = \"600\" height = \"\" alt=\"Fig1\" align=center />\n",
        "Fig 14.4. A typical multi-layer Artificial Neural Network (ANN)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9485c95",
      "metadata": {
        "id": "f9485c95"
      },
      "source": [
        "## 14.7 An overview of TensorFlow and Keras\n",
        "\n",
        "<font color='blue'>TensorFlow</font> is an end-to-end open source platform for machine learning. TensorFlow might be the most popular deep learning library among research engineers and scientists. It has huge community support and great documentation, available at https://www.tensorflow.org/.\n",
        "\n",
        "<font color='blue'>Keras</font> is a high-level deep learning API that can run on top of popular machine learning libraries such as <font color='blue'>TensorFlow</font>, <font color='blue'>Theano</font>, and <font color='blue'>CNTK</font>. <font color='blue'>Keras</font> is very popular in the research and development world because it supports rapid prototyping and provides a user-friendly API to neural network architectures.\n",
        "\n",
        "<font color='blue'>TensorFlow 2</font> introduced great changes in machine learning methods by tightly integrating with <font color='blue'>Keras</font> and providing a high-level API, tf.keras. <font color='blue'>TensorFlow 1</font> was a bit ugly with symbolic graph computations and other low-level computations. With <font color='blue'>TensorFlow 2</font>, developers can take advantage of Keras' user-friendliness as well as TensorFlow's low-level methods.\n",
        "\n",
        "Neural networks are commonly used for computer vision and NLP tasks, including object detection, image classification, and scene understanding as well as text classification, POS tagging, text summarization, and natural language generation.\n",
        "\n",
        "In the following sections, we'll go through the details of a neural network architecture for text classification implemented with <font color='blue'>tf.keras</font>.\n",
        "    \n",
        "Throughout this section, we'll use <font color='blue'>TensorFlow 2</font> as we stated in the Technical requirements section.\n",
        "    \n",
        "Let's warm up to neural networks with some neural network basics, and then start building our <font color='blue'>Keras</font> code."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "487053f9",
      "metadata": {
        "id": "487053f9"
      },
      "source": [
        "## 14.8 Neural Network Layers in Keras\n",
        "\n",
        "A neural network is formed by connecting layers. <font color='blue'>Layers</font> are basically the building blocks of the neural network. A layer consists of several neurons, as in Fig. 4.\n",
        "\n",
        "In Fig. 4, the first layer of this neural network has two layers, and the second layer has six neurons. Each neuron in each layer is connected to all neurons in the next layer.\n",
        "\n",
        "Each layer might have different functionalities; some layers can lower the dimensions of their input, some layers can flatten their input (flattening means collapsing a multidimensional vector into one dimension), and so on. At each layer, we transform the input vectors and feed them to the next layer to get a final vector.   \n",
        "\n",
        "<font color='blue'>Keras</font> provides different sorts of layers, such as <font color='blue'>input layers</font>, <font color='blue'>dense layers</font>, <font color='blue'>dropout layers</font>, <font color='blue'>embedding layers</font>, <font color='blue'>activation layers</font>, <font color='blue'>recurrent layers</font>, and so on. Let's get to know some useful layers one by one:\n",
        "\n",
        "1. Input layer: The input layer is responsible for sending our input data to the rest of the network. While initializing an input layer, we provide the input data shape.\n",
        "2. Dense layers: Dense layers transform the input of a given shape to the output shape we want. Layer 2 in Figure 8.8 represents a dense layer, which collapses a 5-dimensional input into a 1-dimensional output.\n",
        "3. Recurrent layers: Keras provides strong support for RNN, GRU, and LSTM cells. If you're not familiar with RNN variations at all, please refer to the resources in the Technical requirements section. We'll use an LSTM layer in our code. The LSTM layer subsection contains the input and output shape information. In the next subsection, Sequential modeling with LSTMs, we'll get into details of modeling with LSTMs.   \n",
        "4. Dropout layers: Dropout is a technique to prevent overfitting. Overfitting happens when neural networks memorize data instead of learning it. Dropout layers randomly select a given number of neurons and set their weights to zero for the forward and backward passes, that is, for one iteration. We usually place dropout layers after dense layers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93b43520",
      "metadata": {
        "id": "93b43520"
      },
      "source": [
        "These are the basic layers that are used in NLP models. The next subsection is devoted to modeling sequential data with LSTMs, which is the core of statistical modeling for NLP."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6b7c23e",
      "metadata": {
        "id": "a6b7c23e"
      },
      "source": [
        "## 14.8 Sequential Modeling with LSTM Technology\n",
        "\n",
        "<font color='blue'>LSTM (Long-Short-Term-Memory)</font> is an <font color='blue'>RNN (Recurrent Neural Network)</font> variation.\n",
        "\n",
        "<font color='blue'>RNNs</font> are special neural networks that can process sequential data in steps.\n",
        "\n",
        "In usual neural networks, we assume that all the inputs and outputs are independent of each other. Of course, it's not true for text data. Every word's presence depends on the neighbor words.\n",
        "\n",
        "For example, during a machine translation task, we predict a word by considering all the words we predicted before. <font color='blue'>RNNs</font> capture information about the past sequence elements by holding a memory (called hidden state).\n",
        "\n",
        "Fig. 14.5 shows a well-known illustration of <font color='blue'>RNNs</font>. The loop on the left-hand side of the figure explains that an <font color='blue'>RNN</font> feeds the output of the previous step to itself as the current input. The right-hand side of the figure shows the unrolled version of the <font color='blue'>RNN</font> diagram. At each time step, i, we feed the input word xi, and <font color='blue'>RNN</font> outputs a value, hi, for this time step:\n",
        "\n",
        "<img src=\"./Fig 14.5.jpg\" width = \"600\" height = \"\" alt=\"Fig1\" align=center />\n",
        "Fig 14.5. RNN with LSTM Technology\n",
        "\n",
        "<font color='blue'>LSTMs</font> were invented to fix some computational problems of <font color='blue'>RNNs</font> have the problem of forgetting some data back in the sequence, as well as some numerical stability issues due to chain multiplications called vanishing and exploding gradients. If you are interested, you can refer to Colah's blog, the link to which you will find in the References section.\n",
        "\n",
        "An <font color='blue'>LSTM</font> cell is slightly more complicated than an <font color='blue'>RNN</font> cell, but the logic of computation is the same: we feed one input word at each time step and <font color='blue'>LSTM</font> outputs an output value at each time step. The following diagram shows what's inside an <font color='blue'>LSTM</font> cell. Note that the input steps and output steps are identical to the <font color='blue'>RNN</font> counterparts:\n",
        "\n",
        "<img src=\"./Fig 14.6.jpg\" width = \"600\" height = \"\" alt=\"Fig1\" align=center />\n",
        "Fig 14.6. Detail architecture of LSTM cell\n",
        "\n",
        "<font color='blue'>Keras</font> has extensive support for the <font color='blue'>RNN</font> variations <font color='blue'>GRU</font> and <font color='blue'>LSTM</font>, as well as a simple API for training <font color='blue'>RNNs</font>. <font color='blue'>RNN</font> variations are crucial for NLP tasks, as language data's nature is sequential: text is a sequence of words, speech is a sequence of sounds, and so on.\n",
        "\n",
        "Now that we have learned what type of statistical model to use in our design, we can switch to a more practical subject: how to represent a sequence of words. In the next section, we'll learn how to transform a sequence of words into a sequence of word IDs and build vocabularies at the same time with <font color='blue'>Keras's</font> preprocessing module."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a10c35a",
      "metadata": {
        "id": "8a10c35a"
      },
      "source": [
        "## 14.9 Keras Tokenizer in NLP\n",
        "\n",
        "As we remarked in the previous section, text is sequential data (a sequence of words or characters). We'll feed a sentence as a sequence of words. Neural networks can work only with vectors, so we need a way to vectorize the words.\n",
        "\n",
        "In the previous workshop, Working with Word Vectors and Semantic Similarity, we saw how to vectorize words with word vectors. A word vector is a continuous representation of a word. In order to vectorize a word, we follow these steps:\n",
        "\n",
        "1. We tokenize each sentence and turn sentences into a sequence of words.\n",
        "2. We create a vocabulary from the set of words present in step 1. These are words that are supposed to be recognized by our neural network design.\n",
        "3. Creating a vocabulary should assign an ID to each word.\n",
        "4. Then word IDs are mapped to word vectors.\n",
        "\n",
        "Let's look at a short example. We can work on a small corpus of three sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5d27ac1",
      "metadata": {
        "id": "a5d27ac1"
      },
      "outputs": [],
      "source": [
        "testD = [\n",
        "\"I am going to buy some gift for Christmas tomorrow morning.\",\n",
        "\"Yesterday my mom cooked a wonderful meal.\",\n",
        "\"John promised he would remember to turn off the lights.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a82bc22",
      "metadata": {
        "id": "1a82bc22"
      },
      "outputs": [],
      "source": [
        "testD"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "407d797f",
      "metadata": {
        "id": "407d797f"
      },
      "source": [
        "Let's first tokenize the words into sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b9c99b5",
      "metadata": {
        "id": "7b9c99b5"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "utterances = [[token.text for token in nlp(utterance)] for utterance in testD]\n",
        "for utterance in utterances:\n",
        "    utterance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2f93f46",
      "metadata": {
        "id": "a2f93f46"
      },
      "source": [
        "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
        "\n",
        "In the preceding code, we iterated over all tokens of the Doc object generated by calling nlp(sentence). Notice that we didn't filter out the punctuation marks. Filtering punctuation depends on the task. For instance, in sentiment analysis, punctuation marks such as \"!\" correlate to the result. In this example, we'll keep the punctuation marks as well.\n",
        "\n",
        "In the next step, we will try to create vocabularies and token sequences into to-ken-ID sequences by using Tokenizer, as shown:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc705fd8",
      "metadata": {
        "id": "fc705fd8"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "ktoken = Tokenizer(lower=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dee31844",
      "metadata": {
        "id": "dee31844"
      },
      "outputs": [],
      "source": [
        "ktoken.fit_on_texts(testD)\n",
        "ktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d08f21dc",
      "metadata": {
        "id": "d08f21dc"
      },
      "outputs": [],
      "source": [
        "ktoken.word_index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50aa48f6",
      "metadata": {
        "id": "50aa48f6"
      },
      "source": [
        "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
        "\n",
        "In the above codes, we did the following:\n",
        "1. We imported Tokenizer from the Keras text preprocessing module.\n",
        "2. We created a tokenizer object (ktoken) with the parameter lower=True, which means tokenizer should lower all words while building the vocabulary.\n",
        "3. We called ktoken.fit_on_texts on data to build the vocabulary. fit_on_text works on a sequence of tokens; input should al-ways be a list of tokens.\n",
        "4. We examined the vocabulary by printing ktoken.word_index. Word_index is basically a dictionary where keys are vocabu-lary tokens and values are token-IDs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9294ab89",
      "metadata": {
        "id": "9294ab89"
      },
      "source": [
        "To retrieve the token ID, we call ktoken.texts_to_sequences() method.\n",
        "\n",
        "Notice that the input to this method should always be a list, even if we want to feed only one token.\n",
        "\n",
        "In the following code segment, we feed one-word input as a list (notice the list brackets):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d29040b8",
      "metadata": {
        "id": "d29040b8"
      },
      "outputs": [],
      "source": [
        "ktoken.texts_to_sequences([\"Christmas\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5afee47f",
      "metadata": {
        "id": "5afee47f"
      },
      "outputs": [],
      "source": [
        "ktoken.texts_to_sequences([\"cooked\", \"meal\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db480260",
      "metadata": {
        "id": "db480260"
      },
      "source": [
        "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
        "\n",
        "1.\tWe also notice that the token-IDs start from 1, not 0. 0 is a re-served value and has a special meaning, which means a pad-ding value.\n",
        "2.\tKeras cannot process utterances of different lengths, hence we need to pad all the utterances.\n",
        "3.\tWe pad each sentence of the dataset to a maximum length by adding padding utterances either to the start or end of it.\n",
        "4.\tKeras inserts 0 for the padding, which means it's not a real to-ken, but a padding value.\n",
        "\n",
        "Keras inserts 0 for the padding, which means it's not a real word, but a padding value. Let's understand padding with a simple example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e3b1f04",
      "metadata": {
        "id": "5e3b1f04"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "seq_utterance = [[7], [8,1], [9,11,12,14]]\n",
        "\n",
        "# Define Maximum Length (MLEN)\n",
        "MLEN=4\n",
        "pad_sequences(seq_utterance, MLEN, padding=\"post\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebbb45ca",
      "metadata": {
        "id": "ebbb45ca"
      },
      "outputs": [],
      "source": [
        "pad_sequences(seq_utterance, MLEN, padding=\"pre\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f37884f",
      "metadata": {
        "id": "2f37884f"
      },
      "source": [
        "We called pad_sequences on this list of sequences and every sequence is pad-ded with zeros such that its length reaches MAX_LEN=4, the length of the longest sequence.\n",
        "\n",
        "We can pad the sequences from the right or left with the post and pre options. In the preceding code, we padded our sentences with the post option, hence the sentences are padded from the right.\n",
        "\n",
        "If we put it all together, the complete text preprocessing steps are as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88c40779",
      "metadata": {
        "id": "88c40779"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "ktoken = Tokenizer(lower=True)\n",
        "ktoken.fit_on_texts(testD)\n",
        "sutterance = ktoken.texts_to_sequences(testD)\n",
        "MLEN=7\n",
        "pseq_utterance = pad_sequences(sutterance, MLEN, padding=\"post\")\n",
        "pseq_utterance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6343a630",
      "metadata": {
        "id": "6343a630"
      },
      "source": [
        "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
        "\n",
        "Now, we've transformed utterances into a sequence of token-IDs. We've come one step closer to vectorizing the tokens. In the next subsection, we'll finally transform these tokens into vectors. Then our utterances will be ready to be fed into the neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe1e1e15",
      "metadata": {
        "id": "fe1e1e15"
      },
      "source": [
        "### Embedding words\n",
        "\n",
        "We're ready to transform words into word vectors. Embedding words into vectors happens via an <font color='blue'>embedding table</font>.\n",
        "\n",
        "An <font color='blue'>embedding table</font> is basically a lookup table. Each row holds the word vector of a word. We index the rows by <font color='blue'>word-IDs</font>, hence the flow of obtaining a word's word vector is as follows:\n",
        "\n",
        "1. <font color='blue'>word->word-ID</font>: In the previous section, we obtained a <font color='blue'>word-ID</font> for each word with <font color='blue'>Keras' Tokenizer</font>. <font color='blue'>Tokenizer</font> holds all the vocabulary and maps each vocabulary word to an ID, which is an integer.\n",
        "2. <font color='blue'>word-ID->word vector</font>: A <font color='blue'>word-ID</font> is an integer and therefore can be used as an index to the embedding table's rows. Each <font color='blue'>word-ID</font> corresponds to one row and when we want to get a word's word vector, we first obtain its <font color='blue'>word-ID</font> and then do a lookup in the embedding table rows with this word-ID.\n",
        "\n",
        "The following diagram shows how embedding words into word vectors works:\n",
        "\n",
        "<img src=\"./Fig 14.7.jpg\" width = \"600\" height = \"\" alt=\"Fig1\" align=center />\n",
        "Fig 14.7. A sample of embedding words into word vectors\n",
        "\n",
        "Remember that in the previous section, we started with a list of sentences. Then we did the following:\n",
        "1. We broke each sentence into words and built a vocabulary with Keras' Tokenizer.\n",
        "2. The Tokenizer object held a word index, which was a word->word-ID mapping.\n",
        "3. After obtaining the word-ID, we could do a lookup to the embedding table rows with this word-ID and got a word vector.\n",
        "4. Finally, we fed this word vector to the neural network.\n",
        "\n",
        "Training a neural network is not easy. We have to take several steps to transform sentences into vectors.\n",
        "\n",
        "After these preliminary steps, we're ready to design the neural network architecture and do the model training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46200fc6",
      "metadata": {
        "id": "46200fc6"
      },
      "source": [
        "## 14.10 Movie Sentiment Analysis with LTSM using Keras and spaCy\n",
        "\n",
        "In this section, we will design the LSTM-based RNN for our text classifier for sentiment analysis. We'll follow these steps to train the classifier:\n",
        "\n",
        "1.\tData retrieve and preprocessing.\n",
        "2.\tTokenize the review utterances with padding.\n",
        "3.\tCreate the pad sequence of utterances and input them into the Input Lay-er.\n",
        "4.\tVectorize each token by check with the token-ID in the Embedding Layer.\n",
        "5.\tInput these token vectors into the LSTM\n",
        "6.\tTrain the LSTM network\n",
        "\n",
        "Let's get started by remembering the dataset again.\n",
        "\n",
        "\n",
        "### Step 1: Dataset\n",
        "\n",
        "We'll use the same IMDB movie reviews dataset from the sentiment analysis with spaCy section. We already processed the dataset with pandas in that section and reduced it to two columns and binary labels.\n",
        "\n",
        "Reload the reviews table and perform the data preprocessing as done in the pre-vious section to ensure the data is up to date:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa28ab20",
      "metadata": {
        "id": "aa28ab20"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "mcommentDF=pd.read_csv('imdb_5000.csv')\n",
        "mcommentDF = mcommentDF[['Review','Sentiment']].dropna()\n",
        "axplot=mcommentDF.Sentiment.value_counts().plot(kind='bar', colormap='Paired')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "340d4696",
      "metadata": {
        "id": "340d4696"
      },
      "source": [
        "Here are how the mcommentDF dataset looks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dcc119b",
      "metadata": {
        "id": "3dcc119b"
      },
      "outputs": [],
      "source": [
        "mcommentDF.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09cc3e90",
      "metadata": {
        "id": "09cc3e90"
      },
      "source": [
        "Next, we'll transform our dataset a bit. We'll extract the review text and review label from each dataset row and append them into Python lists:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9e8b18a",
      "metadata": {
        "id": "a9e8b18a"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_md\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97e34846",
      "metadata": {
        "id": "97e34846"
      },
      "outputs": [],
      "source": [
        "movie_comment_exp = []\n",
        "categories = []\n",
        "\n",
        "for idx, rw in mcommentDF.iterrows():\n",
        "    comments = rw[\"Review\"]\n",
        "    rating = rw[\"Sentiment\"]\n",
        "    categories.append(rating)\n",
        "    mtoks = [token.text for token in nlp(comments)]\n",
        "    movie_comment_exp.append(mtoks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e85f18f",
      "metadata": {
        "id": "8e85f18f"
      },
      "outputs": [],
      "source": [
        "movie_comment_exp[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "628cce77",
      "metadata": {
        "id": "628cce77"
      },
      "source": [
        "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
        "\n",
        "Notice that we appended a list of words to train_examples, hence each element of this Notice that we appended a list of words to movie_comment_exp, hence each element of this list is a list of tokens. Next, we'll invoke Keras' Tokenizer on this list of tokens to build our vocabulary."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67c43bd6",
      "metadata": {
        "id": "67c43bd6"
      },
      "source": [
        "### Step 2: Data and vocabulary preparation\n",
        "\n",
        "We already processed our dataset, hence we are ready to tokenize the dataset sentences and create a vocabulary. Let's go step by step:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "349418b3",
      "metadata": {
        "id": "349418b3"
      },
      "source": [
        "1. First, we'll do the necessary imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74daf227",
      "metadata": {
        "id": "74daf227"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "725ed5a9",
      "metadata": {
        "id": "725ed5a9"
      },
      "source": [
        "2. We're ready to fit the <font color='blue'>ktoken</font> object on our list of words. First, we'll fit the <font color='blue'>ktoken</font>, then we'll convert words to their IDs by calling <font color='blue'>texts_to_sequences</font>:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ec92e31",
      "metadata": {
        "id": "8ec92e31"
      },
      "outputs": [],
      "source": [
        "ktoken = Tokenizer(lower=True)\n",
        "ktoken.fit_on_texts(movie_comment_exp)\n",
        "\n",
        "seq_utterance = ktoken.texts_to_sequences(movie_comment_exp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "011f028b",
      "metadata": {
        "id": "011f028b"
      },
      "source": [
        "3. Then, we'll pad the short utterance sequences to a maximum length of 50. Also, this will truncate long reviews to a length of 50 words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4a49ca2",
      "metadata": {
        "id": "a4a49ca2"
      },
      "outputs": [],
      "source": [
        "MLEN = 50\n",
        "\n",
        "ps_utterance = pad_sequences(seq_utterance, MLEN, padding=\"post\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c99ed7a0",
      "metadata": {
        "id": "c99ed7a0"
      },
      "source": [
        "4. Finally, we'll convert this list of reviews and the labels to numpy arrays:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7711030f",
      "metadata": {
        "id": "7711030f"
      },
      "outputs": [],
      "source": [
        "ps_utterance = np.array(ps_utterance)\n",
        "catlist = np.array(categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fad5c036",
      "metadata": {
        "id": "fad5c036"
      },
      "outputs": [],
      "source": [
        "catlist = catlist.reshape(catlist.shape[0] , 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcc270c2",
      "metadata": {
        "id": "bcc270c2"
      },
      "outputs": [],
      "source": [
        "catlist.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "736c8d50",
      "metadata": {
        "id": "736c8d50"
      },
      "source": [
        "Up to now, all the basic preparation works are done. We are now ready to create our LSTM network and input data into it.\n",
        "\n",
        "First thing first, load the TensorFlow Keras related modules:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adcb45cc",
      "metadata": {
        "id": "adcb45cc"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "from tensorflow.keras import optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "573d483a",
      "metadata": {
        "id": "573d483a"
      },
      "source": [
        "### Step 3: Implement the Input Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e5bb2bd",
      "metadata": {
        "id": "9e5bb2bd"
      },
      "outputs": [],
      "source": [
        "utterance_input = Input(shape=(None,))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5042dc5b",
      "metadata": {
        "id": "5042dc5b"
      },
      "source": [
        "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
        "\n",
        "Don't be confused by <font color='blue'>None</font> as the input shape. Here, <font color='blue'>None</font> means that this dimension can be any scalar number, hence, we use this expression when we want Keras to infer the input shape."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0e8a10d",
      "metadata": {
        "id": "b0e8a10d"
      },
      "source": [
        "### Step 4: Implement the Embedding Layer\n",
        "\n",
        "Create the Embedding-Layer as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aa70e06",
      "metadata": {
        "id": "2aa70e06"
      },
      "outputs": [],
      "source": [
        "embedding =  Embedding(input_dim = len(ktoken.word_index)+1,\n",
        "                       output_dim = 100)(utterance_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a550e1a",
      "metadata": {
        "id": "9a550e1a"
      },
      "source": [
        "<img src=\"./note.png\" width = \"\" height = \"\"\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "alt=\"note\" align=left />\n",
        "\n",
        "1.\tWhile defining the embedding layer, the input dimension should always be the number of tokens in the vocabulary (here, there's a plus 1 because the indices start from 1, not 0. Index 0 is reserved for the padding value).\n",
        "2.\tHere, we chose the output shape to be 100, hence the token vectors for the vocabulary tokens will be 100-dimensional. Popular numbers for token vector dimensions are 50, 100, and 200 depending on the complexity of the task."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "befa1937",
      "metadata": {
        "id": "befa1937"
      },
      "source": [
        "### Step 5: Implement the LSTM Layer\n",
        "\n",
        "Create the LSTM_Layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f998f8da",
      "metadata": {
        "id": "f998f8da"
      },
      "outputs": [],
      "source": [
        "LSTM_layer = LSTM(units=256)(embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f63e084",
      "metadata": {
        "id": "0f63e084"
      },
      "source": [
        "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
        "\n",
        "Here, the units = 256 is the dimension of the hidden state.\n",
        "The LSTM output shape and hidden state shape are the same due in the LSTM architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef2fd00e",
      "metadata": {
        "id": "ef2fd00e"
      },
      "source": [
        "### Step 6: Implement the Output Layer\n",
        "\n",
        "We obtained a 256-dimensional vector from the LSTM layer and we want to squash it to a 1-dimensional vector (possible values of this vector are 0 and 1, which are the class labels):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5faec640",
      "metadata": {
        "id": "5faec640"
      },
      "outputs": [],
      "source": [
        "outlayer = Dense(1, activation='sigmoid')(LSTM_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2b9b81b",
      "metadata": {
        "id": "d2b9b81b"
      },
      "source": [
        "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
        "\n",
        "We used sigmoid function as the Activation Function in output layer. The sigmoid function is an S-shaped function and maps its input to a [0-1] range and is commonly used in many neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40d940dc",
      "metadata": {
        "id": "40d940dc"
      },
      "source": [
        "### Step 7: System Compilation\n",
        "\n",
        "After defining the model, we need to compile it with an optimizer, a loss function, and an evaluation metric:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ccbe3c1",
      "metadata": {
        "id": "6ccbe3c1"
      },
      "outputs": [],
      "source": [
        "imdb_mdl = Model(inputs=[utterance_input],outputs=[outlayer])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9e8d3fc",
      "metadata": {
        "id": "c9e8d3fc"
      },
      "source": [
        "So far so good.\n",
        "Now, take a look on the imdb_mdl model setup:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "241a9aa2",
      "metadata": {
        "id": "241a9aa2"
      },
      "outputs": [],
      "source": [
        "imdb_mdl.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf5e269f",
      "metadata": {
        "id": "cf5e269f"
      },
      "source": [
        "Next, invoke the model compilation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f63ccd15",
      "metadata": {
        "id": "f63ccd15"
      },
      "outputs": [],
      "source": [
        "imdb_mdl.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebf26805",
      "metadata": {
        "id": "ebf26805"
      },
      "source": [
        "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
        "\n",
        "1.\tFor the imdb_mdl LSTM model, we apply the ADAM (Adap-tive Moment Estimation) as the optimizer for LSTM training, which is commonly used in many LSTM model training.  \n",
        "2.\tAlso, we use binary cross-entropy as loss function, which is also commonly used in many Keras training models. You can find the list of loss functions on the Keras website at https://keras.io/api/losses/.\n",
        "3.\tA list of supported performance metrics can be found in Keras official site at https://keras.io/api/metrics/."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c222c17a",
      "metadata": {
        "id": "c222c17a"
      },
      "source": [
        "### Step 8: Model Fitting and experiment evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6639a5e5",
      "metadata": {
        "id": "6639a5e5"
      },
      "source": [
        "Finally, we'll fit the imdb_mdl model on our data with 5 epochs to save time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49d7642e",
      "metadata": {
        "id": "49d7642e"
      },
      "outputs": [],
      "source": [
        "imdb_mdl.fit(x=ps_utterance,\n",
        "          y=catlist,\n",
        "          batch_size=64,\n",
        "          epochs=5,\n",
        "          validation_split=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55144755",
      "metadata": {
        "id": "55144755"
      },
      "source": [
        "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
        "\n",
        "1.\tHere, x is the list of ps_utterance for network training and y is the list of category (catlist). We want to make 5 passes over the data, hence we set the epochs parameter to 5.\n",
        "2.\tWe went over the data 5 times in batch sizes of 64. Usually, we don't fit all of the dataset into the memory at once (due to memory limitations). Also, the parameter batch_size=64 means we want to feed a batch of 64 training utterances at a time.\n",
        "3.\tThe validation_split = 0.3 is used meaning with we use 70% of the dataset for training and 30% for system validation.\n",
        "4.\tIn summary, our experiment validation accuracy is 0.7793, which is quite good for such a basic LSTM network training for only 5 epochs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25d7ebcc",
      "metadata": {
        "id": "25d7ebcc"
      },
      "source": [
        "<img src=\"./workshop.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
        "\n",
        "### Lab Task 2 Further Exploration of LSTM model for Movie Sentiment Analysis\n",
        "\n",
        "Based on the implementation we have done in this LSTM Lab, we encourage you to experiment more on different settings and simulation.\n",
        "1. Follow the same logic on Workshop 14.1, by using the rating (0 - 10) field of the IMDB movie review dataset. Try to re-construct the LSTM for sentiment analysis of movie review into 3 category: Positive, Neutral and Negative.\n",
        "2. Check the training performance.\n",
        "3. You can experiment with the code more by placing dropout layers at different locations (such as after the embedding layer or after the LSTM layer).\n",
        "4. Another way of experimenting is to try different values for the embedding dimensions, such as 50, 150, and 200, and observe the change in the accuracy.\n",
        "4. The same applies to the LSTM layer's hidden dimension – you can experiment with different values instead of 256.\n",
        "For each,try different parameters and do the simulations, see whether you can find the best configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27656b04",
      "metadata": {
        "id": "27656b04"
      },
      "source": [
        "## 14.10 Summary\n",
        "\n",
        "In this Lab, we have conducted a popular NLP topic – text classification and the application on sentiment analysis.\n",
        "\n",
        "In this workshop, you first learned about text classification concepts such as bi-nary classification, multilabel classification, and multiclass classification.\n",
        "\n",
        "Next, you learned how to train TextCategorizer, spaCy's text classifier compo-nent. You learned how to transform your data into spaCy training format and then train the TextCategorizer component with this data.\n",
        "\n",
        "After learning text classification with spaCy's TextCategorizer, in the final sec-tion, you learned how to combine spaCy code and Keras code.\n",
        "\n",
        "First, you learned the basics of neural networks, including some handy layers such as the dense lay-er, dropout layer, embedding layer, and recurrent layers.\n",
        "\n",
        "Then, you learned how to tokenize and preprocess the data with Keras' Tokenizer.\n",
        "\n",
        "Finally, you went through neural network design with tf.keras code. You learned how to design and evaluate a statistical experiment with LSTM.\n",
        "\n",
        "Looks like a lot! Indeed, it is a lot of material; no worries if it takes time to digest. Practicing text classification can be intense, but in the end, you earn crucial NLP skills!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "211c1dbc",
      "metadata": {
        "id": "211c1dbc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}